{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effecient v2 + Unet++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) load a data for a test\n",
    "2) update a loop for test\n",
    "3) for each epoch display a training loss , validation loss and test loss + ssim value also"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_path = '/home/simlab120/Denoise_comp/Pragyant/image-impeccable-train-data-part1/42487393/seismic_w_noise_vol_42487393.npy'\n",
    "y_train_path = '/home/simlab120/Denoise_comp/Pragyant/image-impeccable-train-data-part1/42487393/seismicCubes_RFC_fullstack_2024.42487393.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_path = '/home/simlab120/Denoise_comp/Pragyant/image-impeccable-test-data/2024-06-10_0d6402b1/seismic_w_noise_vol_44319345.npy'\n",
    "y_test_path = '/home/simlab120/Denoise_comp/Pragyant/image-impeccable-train-data-part1/42487919/seismicCubes_RFC_fullstack_2024.42487919.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_path = '/home/simlab120/Denoise_comp/Pragyant/image-impeccable-train-data-part1/42502775/seismic_w_noise_vol_42502775.npy'\n",
    "y_valid_path = '/home/simlab120/Denoise_comp/Pragyant/image-impeccable-train-data-part1/42502775/seismicCubes_RFC_fullstack_2024.42502775.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (1259,300,300)\n",
    "# load the data\n",
    "# make the data in 2d section\n",
    "# make the train set and test set\n",
    "# print the image also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch\n",
    "# 2 files\n",
    "# 1259*300*300 ---> 1259*300 in 300 images\n",
    "# x,y load\n",
    "# x,y sub sample\n",
    "# pair\n",
    "# return\n",
    "class volDataset(Dataset):\n",
    "    def __init__(self, x_slices, y_slices, transform=None):\n",
    "        \"\"\"\n",
    "        x_slices: List of noisy seismic slices (each of shape 1259x300)\n",
    "        y_slices: List of clean seismic slices (each of shape 1259x300)\n",
    "        \"\"\"\n",
    "        self.x_slices = x_slices\n",
    "        self.y_slices = y_slices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_slices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns one slice (1, 1259, 300)\"\"\"\n",
    "        x_slice = torch.tensor(self.x_slices[idx], dtype=torch.float32).unsqueeze(0)  # (1, 1259, 300)\n",
    "        y_slice = torch.tensor(self.y_slices[idx], dtype=torch.float32).unsqueeze(0)  # (1, 1259, 300)\n",
    "\n",
    "        if self.transform:\n",
    "            x_slice = self.transform(x_slice)\n",
    "            y_slice = self.transform(y_slice)\n",
    "\n",
    "        return x_slice, y_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Batch Shape: torch.Size([8, 1, 1259, 300]) torch.Size([8, 1, 1259, 300])\n",
      "valid Batch Shape: torch.Size([8, 1, 1259, 300]) torch.Size([8, 1, 1259, 300])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Convert volume to 2D slices\n",
    "# Load seismic volumes\n",
    "X_volume = np.load('/home/simlab120/Denoise_comp/Pragyant/image-impeccable-train-data-part1/42487393/seismic_w_noise_vol_42487393.npy',allow_pickle=True)  # (1259, 300, 300)\n",
    "y_volume = np.load('/home/simlab120/Denoise_comp/Pragyant/image-impeccable-train-data-part1/42487393/seismicCubes_RFC_fullstack_2024.42487393.npy',allow_pickle=True)  # (1259, 300, 300)\n",
    "\n",
    "X_volume_t = np.load(X_test_path,allow_pickle=True)\n",
    "y_volume_t = np.load(y_test_path,allow_pickle=True)\n",
    "\n",
    "X_volume_v = np.load(X_valid_path,allow_pickle=True)\n",
    "y_volume_v = np.load(y_valid_path,allow_pickle=True)\n",
    "# Convert volume to 2D slices\n",
    "\n",
    "X_slices = [X_volume[:, :, i] for i in range(X_volume.shape[2])]  # List of 300 slices\n",
    "y_slices = [y_volume[:, :, i] for i in range(y_volume.shape[2])]  # List of 300 slices\n",
    "\n",
    "X_slices_t = [X_volume_t[:, :, i] for i in range(X_volume_t.shape[2])]\n",
    "y_slices_t = [y_volume_t[:, :, i] for i in range(y_volume_t.shape[2])]\n",
    "\n",
    "X_slices_v = [X_volume_v[:, :, i] for i in range(X_volume_v.shape[2])]\n",
    "y_slices_v = [y_volume_v[:, :, i] for i in range(y_volume_v.shape[2])]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = volDataset(X_slices, y_slices)\n",
    "valid_dataset = volDataset(X_slices_v,y_slices_v)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(valid_dataset,batch_size=8, shuffle=False)\n",
    "\n",
    "# Check batch shapes\n",
    "for x_batch, y_batch in train_loader:\n",
    "    print(\"Train Batch Shape:\", x_batch.shape, y_batch.shape)  # Expected: (8, 1, 1259, 300)\n",
    "    break\n",
    "for x_batch, y_batch in val_loader:\n",
    "    print(\"valid Batch Shape:\", x_batch.shape, y_batch.shape)  # Expected: (8, 1, 1259, 300)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crop2d(nn.Module):\n",
    "    def __init__(self, left=10, right=10, top=21, bottom=0):\n",
    "        super(Crop2d, self).__init__()\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.top = top\n",
    "        self.bottom = bottom\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, self.top:x.shape[2] - self.bottom, self.left:x.shape[3] - self.right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in ./anaconda3/envs/torch/lib/python3.12/site-packages (1.0.15)\n",
      "Requirement already satisfied: torch in ./anaconda3/envs/torch/lib/python3.12/site-packages (from timm) (2.4.0)\n",
      "Requirement already satisfied: torchvision in ./anaconda3/envs/torch/lib/python3.12/site-packages (from timm) (0.19.0)\n",
      "Requirement already satisfied: pyyaml in ./anaconda3/envs/torch/lib/python3.12/site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in ./anaconda3/envs/torch/lib/python3.12/site-packages (from timm) (0.29.3)\n",
      "Requirement already satisfied: safetensors in ./anaconda3/envs/torch/lib/python3.12/site-packages (from timm) (0.5.3)\n",
      "Requirement already satisfied: filelock in ./anaconda3/envs/torch/lib/python3.12/site-packages (from huggingface_hub->timm) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from huggingface_hub->timm) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from huggingface_hub->timm) (24.1)\n",
      "Requirement already satisfied: requests in ./anaconda3/envs/torch/lib/python3.12/site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from huggingface_hub->timm) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from huggingface_hub->timm) (4.11.0)\n",
      "Requirement already satisfied: sympy in ./anaconda3/envs/torch/lib/python3.12/site-packages (from torch->timm) (1.12)\n",
      "Requirement already satisfied: networkx in ./anaconda3/envs/torch/lib/python3.12/site-packages (from torch->timm) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from torch->timm) (3.1.4)\n",
      "Requirement already satisfied: setuptools in ./anaconda3/envs/torch/lib/python3.12/site-packages (from torch->timm) (72.1.0)\n",
      "Requirement already satisfied: numpy in ./anaconda3/envs/torch/lib/python3.12/site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from torchvision->timm) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from jinja2->torch->timm) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from sympy->torch->timm) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: segmentation_models_pytorch in ./anaconda3/envs/torch/lib/python3.12/site-packages (0.4.0)\n",
      "Requirement already satisfied: efficientnet-pytorch>=0.6.1 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from segmentation_models_pytorch) (0.7.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from segmentation_models_pytorch) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.19.3 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from segmentation_models_pytorch) (1.26.4)\n",
      "Requirement already satisfied: pillow>=8 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from segmentation_models_pytorch) (10.4.0)\n",
      "Requirement already satisfied: pretrainedmodels>=0.7.1 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from segmentation_models_pytorch) (0.7.4)\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from segmentation_models_pytorch) (1.16.0)\n",
      "Requirement already satisfied: timm>=0.9 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from segmentation_models_pytorch) (1.0.15)\n",
      "Requirement already satisfied: torch>=1.8 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from segmentation_models_pytorch) (2.4.0)\n",
      "Requirement already satisfied: torchvision>=0.9 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from segmentation_models_pytorch) (0.19.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from segmentation_models_pytorch) (4.66.5)\n",
      "Requirement already satisfied: filelock in ./anaconda3/envs/torch/lib/python3.12/site-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (6.0.1)\n",
      "Requirement already satisfied: requests in ./anaconda3/envs/torch/lib/python3.12/site-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (4.11.0)\n",
      "Requirement already satisfied: munch in ./anaconda3/envs/torch/lib/python3.12/site-packages (from pretrainedmodels>=0.7.1->segmentation_models_pytorch) (4.0.0)\n",
      "Requirement already satisfied: safetensors in ./anaconda3/envs/torch/lib/python3.12/site-packages (from timm>=0.9->segmentation_models_pytorch) (0.5.3)\n",
      "Requirement already satisfied: sympy in ./anaconda3/envs/torch/lib/python3.12/site-packages (from torch>=1.8->segmentation_models_pytorch) (1.12)\n",
      "Requirement already satisfied: networkx in ./anaconda3/envs/torch/lib/python3.12/site-packages (from torch>=1.8->segmentation_models_pytorch) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from torch>=1.8->segmentation_models_pytorch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in ./anaconda3/envs/torch/lib/python3.12/site-packages (from torch>=1.8->segmentation_models_pytorch) (72.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from jinja2->torch>=1.8->segmentation_models_pytorch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./anaconda3/envs/torch/lib/python3.12/site-packages (from sympy->torch>=1.8->segmentation_models_pytorch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simlab120/anaconda3/envs/torch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "from segmentation_models_pytorch.decoders.unetplusplus import UnetPlusPlus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given transposed=1, weight of size [2560, 640, 2, 2], expected input[1, 384, 40, 10] to have 2560 channels, but got 384 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m model \u001b[38;5;241m=\u001b[39m UNetPlusPlus()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     85\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1259\u001b[39m, \u001b[38;5;241m300\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# (Batch, Channels, Height, Width)\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Expected output: (1, 1, 1259, 300)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 71\u001b[0m, in \u001b[0;36mUNetPlusPlus.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m e6 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc6(e5)  \u001b[38;5;66;03m# 2560 channels (bottleneck)\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# ✅ Decoder with Skip Connections (Handling mismatch using cropping)\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m d1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me6\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me5\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 640\u001b[39;00m\n\u001b[1;32m     72\u001b[0m d2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup2(d1, e4)  \u001b[38;5;66;03m# 320\u001b[39;00m\n\u001b[1;32m     73\u001b[0m d3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup3(d2, e3)  \u001b[38;5;66;03m# 160\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 20\u001b[0m, in \u001b[0;36mUpBlock.forward\u001b[0;34m(self, x, skip)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, skip):\n\u001b[0;32m---> 20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Upsample\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenter_crop(x, skip)  \u001b[38;5;66;03m# Ensure spatial size matches\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, skip], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Skip connection\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/conv.py:948\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    943\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    944\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    946\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [2560, 640, 2, 2], expected input[1, 384, 40, 10] to have 2560 channels, but got 384 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.models import efficientnet_v2_l, EfficientNet_V2_L_Weights\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsampling block that uses a ConvTranspose2d for upsampling,\n",
    "    center-crops the upsampled feature map to match the skip connection,\n",
    "    concatenates them, and applies a Conv-BN-ReLU sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(out_channels + skip_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        x = self.center_crop(x, skip)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "    def center_crop(self, x, target):\n",
    "        # Custom center crop: crop x to the spatial size of target.\n",
    "        _, _, target_h, target_w = target.shape\n",
    "        _, _, h, w = x.shape\n",
    "        start_h = (h - target_h) // 2\n",
    "        start_w = (w - target_w) // 2\n",
    "        return x[:, :, start_h:start_h + target_h, start_w:start_w + target_w]\n",
    "\n",
    "class UNetPlusPlus(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load EfficientNet_V2_L with pretrained weights.\n",
    "        efficientnet = efficientnet_v2_l(weights=EfficientNet_V2_L_Weights.IMAGENET1K_V1)\n",
    "        self.encoder = efficientnet.features\n",
    "\n",
    "        # Convert the first convolution to accept grayscale (1-channel) input.\n",
    "        first_conv = self.encoder[0][0]\n",
    "        self.encoder[0][0] = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=first_conv.out_channels,\n",
    "            kernel_size=first_conv.kernel_size,\n",
    "            stride=first_conv.stride,\n",
    "            padding=first_conv.padding,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        # Split encoder into stages.\n",
    "        self.enc1 = self.encoder[0]   # e1: 32 channels\n",
    "        self.enc2 = self.encoder[1]   # e2: 32 channels\n",
    "        self.enc3 = self.encoder[2]   # e3: 64 channels\n",
    "        self.enc4 = self.encoder[3]   # e4: 96 channels\n",
    "        self.enc5 = self.encoder[4]   # e5: 192 channels\n",
    "        self.enc6 = self.encoder[5]   # e6: 224 channels (not used in decoder)\n",
    "        self.enc7 = self.encoder[6]   # e7: 384 channels (not used in decoder)\n",
    "        self.enc8 = self.encoder[7]   # e8: 640 channels\n",
    "\n",
    "        # Define decoder upsampling blocks with appropriate channel settings.\n",
    "        self.up1 = UpBlock(in_channels=640, skip_channels=192, out_channels=256)  # e8 upsampled to 256; skip from e5 (192)\n",
    "        self.up2 = UpBlock(in_channels=256, skip_channels=96,  out_channels=160)   # skip from e4 (96)\n",
    "        self.up3 = UpBlock(in_channels=160, skip_channels=64,  out_channels=128)   # skip from e3 (64)\n",
    "        self.up4 = UpBlock(in_channels=128, skip_channels=32,  out_channels=64)    # skip from e2 (32)\n",
    "        self.up5 = UpBlock(in_channels=64,  skip_channels=32,  out_channels=64)    # skip from e1 (32)\n",
    "\n",
    "        # Final 1x1 convolution to produce the segmentation mask (1 channel)\n",
    "        self.final_conv = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder forward pass.\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        e4 = self.enc4(e3)\n",
    "        e5 = self.enc5(e4)\n",
    "        e6 = self.enc6(e5)\n",
    "        e7 = self.enc7(e6)\n",
    "        e8 = self.enc8(e7)\n",
    "\n",
    "        # Decoder with skip connections.\n",
    "        d1 = self.up1(e8, e5)\n",
    "        d2 = self.up2(d1, e4)\n",
    "        d3 = self.up3(d2, e3)\n",
    "        d4 = self.up4(d3, e2)\n",
    "        d5 = self.up5(d4, e1)\n",
    "\n",
    "        out = self.final_conv(d5)\n",
    "        # Upsample the output to match the original input size.\n",
    "        out = F.interpolate(out, size=(x.shape[2], x.shape[3]), mode='bilinear', align_corners=True)\n",
    "        return out\n",
    "\n",
    "# -----------------------------\n",
    "# Test the model with a dummy input.\n",
    "# -----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = UNetPlusPlus().to(device)\n",
    "\n",
    "# Input size is 1259x300\n",
    "x = torch.randn(1, 1, 1259, 300).to(device)\n",
    "out = model(x)\n",
    "print(\"Output shape:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torcheval in ./anaconda3/envs/torch/lib/python3.12/site-packages (0.0.7)\n",
      "Requirement already satisfied: typing-extensions in ./anaconda3/envs/torch/lib/python3.12/site-packages (from torcheval) (4.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torcheval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define model\n",
    "\n",
    "# Define Loss Function (MSE for denoising)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define Optimizer (Adam works well)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Batch Shape: torch.Size([8, 1, 1259, 300]) torch.Size([8, 1, 1259, 300])\n"
     ]
    }
   ],
   "source": [
    "# Check batch shapes\n",
    "for x_batch, y_batch in train_loader:\n",
    "    print(\"Train Batch Shape:\", x_batch.shape, y_batch.shape)  # Expected: (batch_size, 1, 1259, 300)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simlab120/anaconda3/envs/torch/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `StructuralSimilarityIndexMeasure` from `torchmetrics` was deprecated and will be removed in 2.0. Import `StructuralSimilarityIndexMeasure` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics import StructuralSimilarityIndexMeasure\n",
    "ssim_metric = StructuralSimilarityIndexMeasure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "num_epochs = 200\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_ssim = 0.0\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        batch_ssim = ssim_metric(outputs.detach(), y_batch.detach())\n",
    "        running_ssim += batch_ssim.item()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_ssim = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            val_outputs = model(x_val)\n",
    "            val_loss += criterion(val_outputs, y_val).item()\n",
    "            val_ssim += ssim_metric(val_outputs, y_val).item()\n",
    "\n",
    "    # Prediction on a single slice from X_slices_t\n",
    "    random_slice = X_slices_t[torch.randint(0, len(X_slices_t), (1,)).item()]\n",
    "\n",
    "    # Convert to PyTorch tensor and add batch and channel dimensions\n",
    "    random_slice = torch.tensor(random_slice, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    prediction = model(random_slice).detach()\n",
    "\n",
    "    # Visualize the original and predicted slice\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Original Slice\")\n",
    "    plt.imshow(random_slice.squeeze().cpu().numpy(), cmap='seismic')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Predicted Slice\")\n",
    "    plt.imshow(prediction.squeeze().cpu().numpy(), cmap='seismic')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {running_loss / len(train_loader):.4f}, Train SSIM: {running_ssim / len(train_loader):.4f}, \\\n",
    "          Val Loss: {val_loss / len(val_loader):.4f}, Val SSIM: {val_ssim / len(val_loader):.4f}\")\n",
    "\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
