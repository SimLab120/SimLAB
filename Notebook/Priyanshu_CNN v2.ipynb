{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_path = '/home/simlab120/Denoise_comp/Pragyant/image-impeccable-train-data-part1/42487393/seismic_w_noise_vol_42487393.npy'\n",
    "y_train_path = '/home/simlab120/Denoise_comp/Pragyant/image-impeccable-train-data-part1/42487393/seismicCubes_RFC_fullstack_2024.42487393.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (1259,300,300)\n",
    "# load the data\n",
    "# make the data in 2d section\n",
    "# make the train set and test set\n",
    "# print the image also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch\n",
    "# 2 files\n",
    "# 1259*300*300 ---> 1259*300 in 300 images\n",
    "# x,y load\n",
    "# x,y sub sample\n",
    "# pair\n",
    "# return\n",
    "class volDataset(Dataset):\n",
    "    def __init__(self, x_slices, y_slices, transform=None):\n",
    "        \"\"\"\n",
    "        x_slices: List of noisy seismic slices (each of shape 1259x300)\n",
    "        y_slices: List of clean seismic slices (each of shape 1259x300)\n",
    "        \"\"\"\n",
    "        self.x_slices = x_slices\n",
    "        self.y_slices = y_slices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_slices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns one slice (1, 1259, 300)\"\"\"\n",
    "        x_slice = torch.tensor(self.x_slices[idx], dtype=torch.float32).unsqueeze(0)  # (1, 1259, 300)\n",
    "        y_slice = torch.tensor(self.y_slices[idx], dtype=torch.float32).unsqueeze(0)  # (1, 1259, 300)\n",
    "\n",
    "        if self.transform:\n",
    "            x_slice = self.transform(x_slice)\n",
    "            y_slice = self.transform(y_slice)\n",
    "\n",
    "        return x_slice, y_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Batch Shape: torch.Size([8, 1, 1259, 300]) torch.Size([8, 1, 1259, 300])\n",
      "Test Batch Shape: torch.Size([8, 1, 1259, 300]) torch.Size([8, 1, 1259, 300])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Convert volume to 2D slices\n",
    "# Load seismic volumes\n",
    "X_volume = np.load('/home/simlab120/Denoise_comp/Pragyant/image-impeccable-train-data-part1/42487393/seismic_w_noise_vol_42487393.npy',allow_pickle=True)  # (1259, 300, 300)\n",
    "y_volume = np.load('/home/simlab120/Denoise_comp/Pragyant/image-impeccable-train-data-part1/42487393/seismicCubes_RFC_fullstack_2024.42487393.npy',allow_pickle=True)  # (1259, 300, 300)\n",
    "\n",
    "# Convert volume to 2D slices\n",
    "\n",
    "X_slices = [X_volume[:, :, i] for i in range(X_volume.shape[2])]  # List of 300 slices\n",
    "y_slices = [y_volume[:, :, i] for i in range(y_volume.shape[2])]  # List of 300 slices\n",
    "\n",
    "# Train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_slices, y_slices, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = volDataset(X_train, y_train)\n",
    "test_dataset = volDataset(X_test, y_test)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Check batch shapes\n",
    "for x_batch, y_batch in train_loader:\n",
    "    print(\"Train Batch Shape:\", x_batch.shape, y_batch.shape)  # Expected: (8, 1, 1259, 300)\n",
    "    break\n",
    "for x_batch, y_batch in test_loader:\n",
    "    print(\"Test Batch Shape:\", x_batch.shape, y_batch.shape)  # Expected: (8, 1, 1259, 300)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crop2d(nn.Module):\n",
    "    def __init__(self, left=10, right=10, top=21, bottom=0):\n",
    "        super(Crop2d, self).__init__()\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.top = top\n",
    "        self.bottom = bottom\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, self.top:x.shape[2] - self.bottom, self.left:x.shape[3] - self.right]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1259, 300])\n"
     ]
    }
   ],
   "source": [
    "class cnn_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.cnn_model = nn.Sequential(\n",
    "            nn.ZeroPad2d((10,10,21,0)),\n",
    "            nn.Conv2d(in_channels=1, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),  # Corrected\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),  # Corrected\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),  # Corrected\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),  # Corrected\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),  # Corrected\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),  # Corrected\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),  # Corrected\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),  # Corrected\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),  # Corrected\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(),\n",
    "            Crop2d(10, 10, 21, 0),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.cnn_model(x)\n",
    "    \n",
    "model = cnn_model()\n",
    "x = torch.randn(1,1,1259,300)\n",
    "output = model(x)\n",
    "\n",
    "print(output.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torcheval in ./anaconda3/envs/torch/lib/python3.12/site-packages (0.0.7)\n",
      "Requirement already satisfied: typing-extensions in ./anaconda3/envs/torch/lib/python3.12/site-packages (from torcheval) (4.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torcheval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define model\n",
    "model = cnn_model()\n",
    "\n",
    "# Define Loss Function (MSE for denoising)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define Optimizer (Adam works well)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Batch Shape: torch.Size([8, 1, 1259, 300]) torch.Size([8, 1, 1259, 300])\n"
     ]
    }
   ],
   "source": [
    "# Check batch shapes\n",
    "for x_batch, y_batch in train_loader:\n",
    "    print(\"Train Batch Shape:\", x_batch.shape, y_batch.shape)  # Expected: (batch_size, 1, 1259, 300)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import StructuralSimilarityIndexMeasure\n",
    "ssim_metric = StructuralSimilarityIndexMeasure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 10058.8800, SSIM: 0.4739\n",
      "Epoch [2/200], Loss: 9800.5296, SSIM: 0.4736\n",
      "Epoch [3/200], Loss: 9308.3186, SSIM: 0.4755\n",
      "Epoch [4/200], Loss: 8895.1422, SSIM: 0.4763\n",
      "Epoch [5/200], Loss: 8487.4405, SSIM: 0.4797\n",
      "Epoch [6/200], Loss: 8141.0451, SSIM: 0.4816\n",
      "Epoch [7/200], Loss: 7819.5312, SSIM: 0.4853\n",
      "Epoch [8/200], Loss: 7540.9439, SSIM: 0.4896\n",
      "Epoch [9/200], Loss: 7314.0978, SSIM: 0.4926\n",
      "Epoch [10/200], Loss: 7057.6506, SSIM: 0.4980\n",
      "Epoch [11/200], Loss: 6852.3906, SSIM: 0.5017\n",
      "Epoch [12/200], Loss: 6690.3274, SSIM: 0.5044\n",
      "Epoch [13/200], Loss: 6477.1666, SSIM: 0.5076\n",
      "Epoch [14/200], Loss: 6329.4597, SSIM: 0.5101\n",
      "Epoch [15/200], Loss: 6320.7338, SSIM: 0.5109\n",
      "Epoch [16/200], Loss: 6142.0287, SSIM: 0.5153\n",
      "Epoch [17/200], Loss: 5958.9480, SSIM: 0.5178\n",
      "Epoch [18/200], Loss: 5942.0502, SSIM: 0.5199\n",
      "Epoch [19/200], Loss: 5771.5368, SSIM: 0.5258\n",
      "Epoch [20/200], Loss: 5659.3751, SSIM: 0.5291\n",
      "Epoch [21/200], Loss: 5615.6192, SSIM: 0.5304\n",
      "Epoch [22/200], Loss: 5541.1966, SSIM: 0.5345\n",
      "Epoch [23/200], Loss: 5556.7079, SSIM: 0.5352\n",
      "Epoch [24/200], Loss: 5379.1754, SSIM: 0.5401\n",
      "Epoch [25/200], Loss: 5376.1848, SSIM: 0.5405\n",
      "Epoch [26/200], Loss: 5281.0571, SSIM: 0.5442\n",
      "Epoch [27/200], Loss: 5215.3613, SSIM: 0.5456\n",
      "Epoch [28/200], Loss: 5181.9816, SSIM: 0.5480\n",
      "Epoch [29/200], Loss: 5129.2520, SSIM: 0.5484\n",
      "Epoch [30/200], Loss: 5356.7920, SSIM: 0.5450\n",
      "Epoch [31/200], Loss: 5157.2931, SSIM: 0.5491\n",
      "Epoch [32/200], Loss: 5069.8521, SSIM: 0.5535\n",
      "Epoch [33/200], Loss: 5017.8369, SSIM: 0.5543\n",
      "Epoch [34/200], Loss: 5040.3447, SSIM: 0.5542\n",
      "Epoch [35/200], Loss: 4982.4290, SSIM: 0.5572\n",
      "Epoch [36/200], Loss: 4920.2412, SSIM: 0.5603\n",
      "Epoch [37/200], Loss: 4901.3047, SSIM: 0.5604\n",
      "Epoch [38/200], Loss: 4904.2562, SSIM: 0.5606\n",
      "Epoch [39/200], Loss: 4875.4842, SSIM: 0.5617\n",
      "Epoch [40/200], Loss: 4879.7897, SSIM: 0.5621\n",
      "Epoch [41/200], Loss: 4865.2953, SSIM: 0.5632\n",
      "Epoch [42/200], Loss: 4820.4188, SSIM: 0.5650\n",
      "Epoch [43/200], Loss: 4800.8543, SSIM: 0.5664\n",
      "Epoch [44/200], Loss: 4781.4130, SSIM: 0.5662\n",
      "Epoch [45/200], Loss: 4805.3226, SSIM: 0.5647\n",
      "Epoch [46/200], Loss: 4786.5754, SSIM: 0.5669\n",
      "Epoch [47/200], Loss: 4723.6197, SSIM: 0.5694\n",
      "Epoch [48/200], Loss: 4736.5478, SSIM: 0.5684\n",
      "Epoch [49/200], Loss: 4730.9143, SSIM: 0.5707\n",
      "Epoch [50/200], Loss: 4699.9961, SSIM: 0.5701\n",
      "Epoch [51/200], Loss: 4686.5968, SSIM: 0.5722\n",
      "Epoch [52/200], Loss: 4715.4435, SSIM: 0.5706\n",
      "Epoch [53/200], Loss: 4717.4808, SSIM: 0.5700\n",
      "Epoch [54/200], Loss: 4800.3979, SSIM: 0.5689\n",
      "Epoch [55/200], Loss: 4685.7781, SSIM: 0.5718\n",
      "Epoch [56/200], Loss: 4671.1674, SSIM: 0.5731\n",
      "Epoch [57/200], Loss: 4623.0699, SSIM: 0.5748\n",
      "Epoch [58/200], Loss: 4639.3557, SSIM: 0.5748\n",
      "Epoch [59/200], Loss: 4621.9020, SSIM: 0.5767\n",
      "Epoch [60/200], Loss: 4586.2558, SSIM: 0.5768\n",
      "Epoch [61/200], Loss: 4606.8992, SSIM: 0.5776\n",
      "Epoch [62/200], Loss: 4652.7691, SSIM: 0.5743\n",
      "Epoch [63/200], Loss: 4595.1564, SSIM: 0.5762\n",
      "Epoch [64/200], Loss: 4564.8551, SSIM: 0.5785\n",
      "Epoch [65/200], Loss: 4546.3618, SSIM: 0.5790\n",
      "Epoch [66/200], Loss: 4596.1550, SSIM: 0.5786\n",
      "Epoch [67/200], Loss: 4548.1122, SSIM: 0.5799\n",
      "Epoch [68/200], Loss: 4550.3894, SSIM: 0.5804\n",
      "Epoch [69/200], Loss: 4570.1139, SSIM: 0.5788\n",
      "Epoch [70/200], Loss: 4561.7925, SSIM: 0.5795\n",
      "Epoch [71/200], Loss: 4567.5762, SSIM: 0.5790\n",
      "Epoch [72/200], Loss: 4532.0562, SSIM: 0.5806\n",
      "Epoch [73/200], Loss: 4546.6192, SSIM: 0.5809\n",
      "Epoch [74/200], Loss: 4563.1189, SSIM: 0.5807\n",
      "Epoch [75/200], Loss: 4546.9676, SSIM: 0.5814\n",
      "Epoch [76/200], Loss: 4552.2319, SSIM: 0.5812\n",
      "Epoch [77/200], Loss: 4541.1342, SSIM: 0.5824\n",
      "Epoch [78/200], Loss: 4501.1191, SSIM: 0.5839\n",
      "Epoch [79/200], Loss: 4493.0893, SSIM: 0.5836\n",
      "Epoch [80/200], Loss: 4513.2453, SSIM: 0.5828\n",
      "Epoch [81/200], Loss: 4521.7839, SSIM: 0.5832\n",
      "Epoch [82/200], Loss: 4513.4446, SSIM: 0.5841\n",
      "Epoch [83/200], Loss: 4475.5198, SSIM: 0.5841\n",
      "Epoch [84/200], Loss: 4494.4547, SSIM: 0.5855\n",
      "Epoch [85/200], Loss: 4490.0846, SSIM: 0.5843\n",
      "Epoch [86/200], Loss: 4475.7721, SSIM: 0.5840\n",
      "Epoch [87/200], Loss: 4495.5142, SSIM: 0.5856\n",
      "Epoch [88/200], Loss: 4548.5494, SSIM: 0.5827\n",
      "Epoch [89/200], Loss: 4543.7019, SSIM: 0.5830\n",
      "Epoch [90/200], Loss: 4472.2552, SSIM: 0.5857\n",
      "Epoch [91/200], Loss: 4517.1839, SSIM: 0.5859\n",
      "Epoch [92/200], Loss: 4479.4981, SSIM: 0.5843\n",
      "Epoch [93/200], Loss: 4451.2606, SSIM: 0.5866\n",
      "Epoch [94/200], Loss: 4470.9960, SSIM: 0.5862\n",
      "Epoch [95/200], Loss: 4445.9679, SSIM: 0.5872\n",
      "Epoch [96/200], Loss: 4460.0115, SSIM: 0.5868\n",
      "Epoch [97/200], Loss: 4437.8760, SSIM: 0.5877\n",
      "Epoch [98/200], Loss: 4441.1611, SSIM: 0.5852\n",
      "Epoch [99/200], Loss: 4428.5455, SSIM: 0.5876\n",
      "Epoch [100/200], Loss: 4441.9838, SSIM: 0.5880\n",
      "Epoch [101/200], Loss: 4442.5009, SSIM: 0.5884\n",
      "Epoch [102/200], Loss: 4469.2726, SSIM: 0.5871\n",
      "Epoch [103/200], Loss: 4417.0544, SSIM: 0.5889\n",
      "Epoch [104/200], Loss: 4409.0674, SSIM: 0.5889\n",
      "Epoch [105/200], Loss: 4429.5718, SSIM: 0.5900\n",
      "Epoch [106/200], Loss: 4398.7566, SSIM: 0.5897\n",
      "Epoch [107/200], Loss: 4409.4415, SSIM: 0.5901\n",
      "Epoch [108/200], Loss: 4426.2884, SSIM: 0.5897\n",
      "Epoch [109/200], Loss: 4439.7833, SSIM: 0.5890\n",
      "Epoch [110/200], Loss: 4419.2505, SSIM: 0.5902\n",
      "Epoch [111/200], Loss: 4449.3976, SSIM: 0.5897\n",
      "Epoch [112/200], Loss: 4443.0878, SSIM: 0.5889\n",
      "Epoch [113/200], Loss: 4434.7865, SSIM: 0.5891\n",
      "Epoch [114/200], Loss: 4427.6091, SSIM: 0.5891\n",
      "Epoch [115/200], Loss: 4384.4006, SSIM: 0.5920\n",
      "Epoch [116/200], Loss: 4396.1566, SSIM: 0.5909\n",
      "Epoch [117/200], Loss: 4448.3559, SSIM: 0.5897\n",
      "Epoch [118/200], Loss: 4411.6745, SSIM: 0.5917\n",
      "Epoch [119/200], Loss: 4398.0166, SSIM: 0.5912\n",
      "Epoch [120/200], Loss: 4420.5171, SSIM: 0.5899\n",
      "Epoch [121/200], Loss: 4405.0315, SSIM: 0.5915\n",
      "Epoch [122/200], Loss: 4387.4172, SSIM: 0.5919\n",
      "Epoch [123/200], Loss: 4377.1857, SSIM: 0.5925\n",
      "Epoch [124/200], Loss: 4375.3462, SSIM: 0.5920\n",
      "Epoch [125/200], Loss: 4385.6099, SSIM: 0.5921\n",
      "Epoch [126/200], Loss: 4396.4332, SSIM: 0.5926\n",
      "Epoch [127/200], Loss: 4373.6207, SSIM: 0.5922\n",
      "Epoch [128/200], Loss: 4389.6476, SSIM: 0.5936\n",
      "Epoch [129/200], Loss: 4368.6896, SSIM: 0.5927\n",
      "Epoch [130/200], Loss: 4353.7648, SSIM: 0.5938\n",
      "Epoch [131/200], Loss: 4363.8396, SSIM: 0.5937\n",
      "Epoch [132/200], Loss: 4368.9166, SSIM: 0.5929\n",
      "Epoch [133/200], Loss: 4361.7173, SSIM: 0.5938\n",
      "Epoch [134/200], Loss: 4375.7442, SSIM: 0.5939\n",
      "Epoch [135/200], Loss: 4377.9955, SSIM: 0.5932\n",
      "Epoch [136/200], Loss: 4371.9201, SSIM: 0.5931\n",
      "Epoch [137/200], Loss: 4371.0747, SSIM: 0.5936\n",
      "Epoch [138/200], Loss: 4385.8880, SSIM: 0.5924\n",
      "Epoch [139/200], Loss: 4395.9123, SSIM: 0.5925\n",
      "Epoch [140/200], Loss: 4396.5243, SSIM: 0.5927\n",
      "Epoch [141/200], Loss: 4351.8648, SSIM: 0.5943\n",
      "Epoch [142/200], Loss: 4358.0198, SSIM: 0.5946\n",
      "Epoch [143/200], Loss: 4386.7736, SSIM: 0.5929\n",
      "Epoch [144/200], Loss: 4354.5263, SSIM: 0.5952\n",
      "Epoch [145/200], Loss: 4346.6717, SSIM: 0.5943\n",
      "Epoch [146/200], Loss: 4355.2042, SSIM: 0.5950\n",
      "Epoch [147/200], Loss: 4378.1806, SSIM: 0.5949\n",
      "Epoch [148/200], Loss: 4367.5817, SSIM: 0.5939\n",
      "Epoch [149/200], Loss: 4376.3386, SSIM: 0.5946\n",
      "Epoch [150/200], Loss: 4354.3414, SSIM: 0.5952\n",
      "Epoch [151/200], Loss: 4340.3705, SSIM: 0.5953\n",
      "Epoch [152/200], Loss: 4317.6459, SSIM: 0.5972\n",
      "Epoch [153/200], Loss: 4322.9924, SSIM: 0.5960\n",
      "Epoch [154/200], Loss: 4384.7140, SSIM: 0.5954\n",
      "Epoch [155/200], Loss: 4392.4086, SSIM: 0.5937\n",
      "Epoch [156/200], Loss: 4345.6798, SSIM: 0.5949\n",
      "Epoch [157/200], Loss: 4330.4336, SSIM: 0.5960\n",
      "Epoch [158/200], Loss: 4339.0584, SSIM: 0.5949\n",
      "Epoch [159/200], Loss: 4328.9776, SSIM: 0.5963\n",
      "Epoch [160/200], Loss: 4319.0187, SSIM: 0.5970\n",
      "Epoch [161/200], Loss: 4335.8730, SSIM: 0.5973\n",
      "Epoch [162/200], Loss: 4336.8503, SSIM: 0.5968\n",
      "Epoch [163/200], Loss: 4345.6450, SSIM: 0.5960\n",
      "Epoch [164/200], Loss: 4370.9831, SSIM: 0.5956\n",
      "Epoch [165/200], Loss: 4356.2967, SSIM: 0.5948\n",
      "Epoch [166/200], Loss: 4334.4319, SSIM: 0.5964\n",
      "Epoch [167/200], Loss: 4305.9237, SSIM: 0.5977\n",
      "Epoch [168/200], Loss: 4297.8891, SSIM: 0.5971\n",
      "Epoch [169/200], Loss: 4304.4750, SSIM: 0.5970\n",
      "Epoch [170/200], Loss: 4307.7317, SSIM: 0.5978\n",
      "Epoch [171/200], Loss: 4336.6268, SSIM: 0.5973\n",
      "Epoch [172/200], Loss: 4335.5668, SSIM: 0.5972\n",
      "Epoch [173/200], Loss: 4308.0646, SSIM: 0.5977\n",
      "Epoch [174/200], Loss: 4303.3610, SSIM: 0.5995\n",
      "Epoch [175/200], Loss: 4361.9304, SSIM: 0.5968\n",
      "Epoch [176/200], Loss: 4316.8011, SSIM: 0.5968\n",
      "Epoch [177/200], Loss: 4309.4768, SSIM: 0.5979\n",
      "Epoch [178/200], Loss: 4317.9571, SSIM: 0.5974\n",
      "Epoch [179/200], Loss: 4326.6635, SSIM: 0.5975\n",
      "Epoch [180/200], Loss: 4305.9301, SSIM: 0.5982\n",
      "Epoch [181/200], Loss: 4325.7752, SSIM: 0.5969\n",
      "Epoch [182/200], Loss: 4311.3913, SSIM: 0.5984\n",
      "Epoch [183/200], Loss: 4310.7291, SSIM: 0.5985\n",
      "Epoch [184/200], Loss: 4322.0661, SSIM: 0.5986\n",
      "Epoch [185/200], Loss: 4324.9297, SSIM: 0.5985\n",
      "Epoch [186/200], Loss: 4337.0266, SSIM: 0.5980\n",
      "Epoch [187/200], Loss: 4306.0050, SSIM: 0.5982\n",
      "Epoch [188/200], Loss: 4288.2017, SSIM: 0.6002\n",
      "Epoch [189/200], Loss: 4285.1537, SSIM: 0.5999\n",
      "Epoch [190/200], Loss: 4321.2315, SSIM: 0.5981\n",
      "Epoch [191/200], Loss: 4299.0287, SSIM: 0.5986\n",
      "Epoch [192/200], Loss: 4295.1183, SSIM: 0.5981\n",
      "Epoch [193/200], Loss: 4294.7038, SSIM: 0.6002\n",
      "Epoch [194/200], Loss: 4373.1089, SSIM: 0.5964\n",
      "Epoch [195/200], Loss: 4370.2727, SSIM: 0.5971\n",
      "Epoch [196/200], Loss: 4292.4579, SSIM: 0.5989\n",
      "Epoch [197/200], Loss: 4288.5287, SSIM: 0.5983\n",
      "Epoch [198/200], Loss: 4305.2403, SSIM: 0.5989\n",
      "Epoch [199/200], Loss: 4337.9734, SSIM: 0.5987\n",
      "Epoch [200/200], Loss: 4310.8275, SSIM: 0.5990\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200  # Change as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    running_ssim = 0.0\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "        outputs = model(x_batch)  # Forward pass\n",
    "        loss = criterion(outputs, y_batch)  # Compute loss\n",
    "\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Compute SSIM (detach to avoid tracking gradients)\n",
    "        batch_ssim = ssim_metric(outputs.detach(), y_batch.detach())\n",
    "        running_ssim += batch_ssim.item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Print epoch loss\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}, SSIM: {running_ssim/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
