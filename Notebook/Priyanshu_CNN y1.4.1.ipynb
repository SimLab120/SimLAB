{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIT + Unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) load a data for a test\n",
    "2) update a loop for test\n",
    "3) for each epoch display a training loss , validation loss and test loss + ssim value also"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_path = '/home/simlab120/Denoise_comp/Pragyant/image-impeccable-train-data-part1/42487393/seismic_w_noise_vol_42487393.npy'\n",
    "y_train_path = '/home/simlab120/Denoise_comp/Pragyant/image-impeccable-train-data-part1/42487393/seismicCubes_RFC_fullstack_2024.42487393.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_path = '/home/simlab120/Denoise_comp/Pragyant/image-impeccable-test-data/2024-06-10_0d6402b1/seismic_w_noise_vol_44319345.npy'\n",
    "y_test_path = '/home/simlab120/Denoise_comp/Pragyant/image-impeccable-train-data-part1/42487919/seismicCubes_RFC_fullstack_2024.42487919.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_path = '/home/simlab120/Denoise_comp/Pragyant/image-impeccable-train-data-part1/42502775/seismic_w_noise_vol_42502775.npy'\n",
    "y_valid_path = '/home/simlab120/Denoise_comp/Pragyant/image-impeccable-train-data-part1/42502775/seismicCubes_RFC_fullstack_2024.42502775.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (1259,300,300)\n",
    "# load the data\n",
    "# make the data in 2d section\n",
    "# make the train set and test set\n",
    "# print the image also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch\n",
    "# 2 files\n",
    "# 1259*300*300 ---> 1259*300 in 300 images\n",
    "# x,y load\n",
    "# x,y sub sample\n",
    "# pair\n",
    "# return\n",
    "class volDataset(Dataset):\n",
    "    def __init__(self, x_slices, y_slices, transform=None):\n",
    "        \"\"\"\n",
    "        x_slices: List of noisy seismic slices (each of shape 1259x300)\n",
    "        y_slices: List of clean seismic slices (each of shape 1259x300)\n",
    "        \"\"\"\n",
    "        self.x_slices = x_slices\n",
    "        self.y_slices = y_slices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_slices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns one slice (1, 1259, 300)\"\"\"\n",
    "        x_slice = torch.tensor(self.x_slices[idx], dtype=torch.float32).unsqueeze(0)  # (1, 1259, 300)\n",
    "        y_slice = torch.tensor(self.y_slices[idx], dtype=torch.float32).unsqueeze(0)  # (1, 1259, 300)\n",
    "\n",
    "        if self.transform:\n",
    "            x_slice = self.transform(x_slice)\n",
    "            y_slice = self.transform(y_slice)\n",
    "\n",
    "        return x_slice, y_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Batch Shape: torch.Size([8, 1, 1259, 300]) torch.Size([8, 1, 1259, 300])\n",
      "valid Batch Shape: torch.Size([8, 1, 1259, 300]) torch.Size([8, 1, 1259, 300])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Convert volume to 2D slices\n",
    "# Load seismic volumes\n",
    "X_volume = np.load('/home/simlab120/Denoise_comp/Pragyant/image-impeccable-train-data-part1/42487393/seismic_w_noise_vol_42487393.npy',allow_pickle=True)  # (1259, 300, 300)\n",
    "y_volume = np.load('/home/simlab120/Denoise_comp/Pragyant/image-impeccable-train-data-part1/42487393/seismicCubes_RFC_fullstack_2024.42487393.npy',allow_pickle=True)  # (1259, 300, 300)\n",
    "\n",
    "X_volume_t = np.load(X_test_path,allow_pickle=True)\n",
    "y_volume_t = np.load(y_test_path,allow_pickle=True)\n",
    "\n",
    "X_volume_v = np.load(X_valid_path,allow_pickle=True)\n",
    "y_volume_v = np.load(y_valid_path,allow_pickle=True)\n",
    "# Convert volume to 2D slices\n",
    "\n",
    "X_slices = [X_volume[:, :, i] for i in range(X_volume.shape[2])]  # List of 300 slices\n",
    "y_slices = [y_volume[:, :, i] for i in range(y_volume.shape[2])]  # List of 300 slices\n",
    "\n",
    "X_slices_t = [X_volume_t[:, :, i] for i in range(X_volume_t.shape[2])]\n",
    "y_slices_t = [y_volume_t[:, :, i] for i in range(y_volume_t.shape[2])]\n",
    "\n",
    "X_slices_v = [X_volume_v[:, :, i] for i in range(X_volume_v.shape[2])]\n",
    "y_slices_v = [y_volume_v[:, :, i] for i in range(y_volume_v.shape[2])]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = volDataset(X_slices, y_slices)\n",
    "valid_dataset = volDataset(X_slices_v,y_slices_v)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(valid_dataset,batch_size=8, shuffle=False)\n",
    "\n",
    "# Check batch shapes\n",
    "for x_batch, y_batch in train_loader:\n",
    "    print(\"Train Batch Shape:\", x_batch.shape, y_batch.shape)  # Expected: (8, 1, 1259, 300)\n",
    "    break\n",
    "for x_batch, y_batch in val_loader:\n",
    "    print(\"valid Batch Shape:\", x_batch.shape, y_batch.shape)  # Expected: (8, 1, 1259, 300)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crop2d(nn.Module):\n",
    "    def __init__(self, left=10, right=10, top=21, bottom=0):\n",
    "        super(Crop2d, self).__init__()\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.top = top\n",
    "        self.bottom = bottom\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, self.top:x.shape[2] - self.bottom, self.left:x.shape[3] - self.right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simlab120/anaconda3/envs/torch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "from segmentation_models_pytorch.decoders.unetplusplus import UnetPlusPlus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1259, 300])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.vision_transformer import vit_h_14, ViT_H_14_Weights\n",
    "\n",
    "\n",
    "class Crop2d(nn.Module):\n",
    "    \"\"\"Crops a fixed amount from each side of a 4D tensor (B, C, H, W).\"\"\"\n",
    "    def __init__(self, left, right, top, bottom):\n",
    "        super().__init__()\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.top = top\n",
    "        self.bottom = bottom\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, self.top:x.shape[2]-self.bottom, self.left:x.shape[3]-self.right]\n",
    "\n",
    "\n",
    "class ViTUNet(nn.Module):\n",
    "    \"\"\"ViT-H/14 as encoder without UNet decoder, using learned positional embeddings.\"\"\"\n",
    "    def __init__(self, in_channels=1, num_classes=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load pretrained ViT-H/14\n",
    "        vit = vit_h_14(weights=ViT_H_14_Weights.IMAGENET1K_SWAG_E2E_V1)\n",
    "        self.encoder = vit\n",
    "\n",
    "        # Modify conv_proj to accept single-channel images.\n",
    "        self.encoder.conv_proj = nn.Conv2d(in_channels, 1280, kernel_size=14, stride=14, padding=0)\n",
    "\n",
    "        # Adjust zero padding and cropping (Left, Right, Top, Bottom)\n",
    "        self.zero_pad = nn.ZeroPad2d((10, 10, 13, 0))\n",
    "        self.crop = Crop2d(10, 10, 13, 0)\n",
    "\n",
    "        # Final classifier head: Map 1280 channels to 1 (grayscale output)\n",
    "        self.head = nn.Conv2d(1280, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, orig_H, orig_W = x.shape\n",
    "\n",
    "        # Zero-pad input\n",
    "        x_padded = self.zero_pad(x)  # Shape: (B, C, orig_H+13, orig_W+20)\n",
    "\n",
    "        # Conv projection: produces (B, 1280, H_p, W_p)\n",
    "        features = self.encoder.conv_proj(x_padded)\n",
    "        B, C, H_p, W_p = features.shape  # Expect C == 1280\n",
    "\n",
    "        # Flatten spatial dimensions → (B, N, 1280)\n",
    "        features = features.flatten(2).transpose(1, 2)\n",
    "\n",
    "        # Prepend class token → (B, N+1, 1280)\n",
    "        class_token = self.encoder.class_token.expand(B, -1, -1)\n",
    "        tokens = torch.cat([class_token, features], dim=1)\n",
    "\n",
    "        # Desired token count:\n",
    "        new_seq_len = 1 + (H_p * W_p)\n",
    "\n",
    "        # ---- Update the transformer encoder's positional embeddings ----\n",
    "        if hasattr(self.encoder.encoder, 'pos_embedding'):\n",
    "            orig_pos_embed = self.encoder.encoder.pos_embedding  # shape: (1, orig_seq_len, 1280)\n",
    "        else:\n",
    "            raise KeyError(\"Transformer encoder has no 'pos_embedding' attribute.\")\n",
    "\n",
    "        if orig_pos_embed.shape[1] != new_seq_len:\n",
    "            class_pos_embed = orig_pos_embed[:, :1, :]  # (1,1,1280)\n",
    "            patch_pos_embed = orig_pos_embed[:, 1:, :]    # (1, orig_N, 1280)\n",
    "            orig_N = patch_pos_embed.shape[1]\n",
    "            orig_grid_size = int(orig_N ** 0.5)  # assume square grid\n",
    "            patch_pos_embed = patch_pos_embed.reshape(1, orig_grid_size, orig_grid_size, C).permute(0, 3, 1, 2)\n",
    "            new_grid = (H_p, W_p)\n",
    "            patch_pos_embed = F.interpolate(patch_pos_embed, size=new_grid, mode='bilinear', align_corners=False)\n",
    "            patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).reshape(1, -1, C)\n",
    "            new_pos_embed = torch.cat([class_pos_embed, patch_pos_embed], dim=1)\n",
    "        else:\n",
    "            new_pos_embed = orig_pos_embed\n",
    "\n",
    "        self.encoder.encoder.pos_embedding = nn.Parameter(new_pos_embed)\n",
    "\n",
    "        # Add positional embedding to tokens.\n",
    "        tokens = tokens + self.encoder.encoder.pos_embedding\n",
    "\n",
    "        # Transformer encoding.\n",
    "        tokens = self.encoder.encoder(tokens)  # (B, N+1, 1280)\n",
    "\n",
    "        # Remove class token and reshape to (B, 1280, H_p, W_p)\n",
    "        patch_tokens = tokens[:, 1:, :].transpose(1, 2).reshape(B, C, H_p, W_p)\n",
    "\n",
    "        # Resize output back to padded dimensions.\n",
    "        x_out = F.interpolate(patch_tokens, size=(x_padded.shape[2], x_padded.shape[3]),\n",
    "                              mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        # Crop back to original dimensions.\n",
    "        x_out = self.crop(x_out)\n",
    "\n",
    "        # Final classifier head: map 1280 channels to 1\n",
    "        x_out = self.head(x_out)\n",
    "\n",
    "        return x_out\n",
    "\n",
    "\n",
    "# Test the model\n",
    "model = ViTUNet()\n",
    "x = torch.randn(1, 1, 1259, 300)  # (B, Channels, H, W)\n",
    "out = model(x)\n",
    "print(out.shape)  # Expected output: (1, 1, 1259, 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torcheval in ./anaconda3/envs/torch/lib/python3.12/site-packages (0.0.7)\n",
      "Requirement already satisfied: typing-extensions in ./anaconda3/envs/torch/lib/python3.12/site-packages (from torcheval) (4.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torcheval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define model\n",
    "model = ViTUNet()\n",
    "\n",
    "# Define Loss Function (MSE for denoising)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define Optimizer (Adam works well)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Batch Shape: torch.Size([8, 1, 1259, 300]) torch.Size([8, 1, 1259, 300])\n"
     ]
    }
   ],
   "source": [
    "# Check batch shapes\n",
    "for x_batch, y_batch in train_loader:\n",
    "    print(\"Train Batch Shape:\", x_batch.shape, y_batch.shape)  # Expected: (batch_size, 1, 1259, 300)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simlab120/anaconda3/envs/torch/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `StructuralSimilarityIndexMeasure` from `torchmetrics` was deprecated and will be removed in 2.0. Import `StructuralSimilarityIndexMeasure` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics import StructuralSimilarityIndexMeasure\n",
    "ssim_metric = StructuralSimilarityIndexMeasure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 16672358400 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_batch)\n\u001b[1;32m     14\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 90\u001b[0m, in \u001b[0;36mViTUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     87\u001b[0m patch_tokens \u001b[38;5;241m=\u001b[39m tokens[:, \u001b[38;5;241m1\u001b[39m:, :]\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, C, H_p, W_p)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Resize output back to padded dimensions.\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m x_out \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_padded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_padded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbilinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Crop back to original dimensions.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m x_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrop(x_out)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/functional.py:4087\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4081\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mare_deterministic_algorithms_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mis_cuda:\n\u001b[1;32m   4082\u001b[0m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put\u001b[39;00m\n\u001b[1;32m   4083\u001b[0m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[1;32m   4084\u001b[0m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[1;32m   4085\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch._decomp.decompositions\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39m_upsample_linear_vec(\n\u001b[1;32m   4086\u001b[0m                 \u001b[38;5;28minput\u001b[39m, output_size, align_corners, scale_factors)\n\u001b[0;32m-> 4087\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsample_bilinear2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m align_corners \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 16672358400 bytes. Error code 12 (Cannot allocate memory)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Number of epochs\n",
    "num_epochs = 200\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_ssim = 0.0\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        batch_ssim = ssim_metric(outputs.detach(), y_batch.detach())\n",
    "        running_ssim += batch_ssim.item()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_ssim = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            val_outputs = model(x_val)\n",
    "            val_loss += criterion(val_outputs, y_val).item()\n",
    "            val_ssim += ssim_metric(val_outputs, y_val).item()\n",
    "\n",
    "    # Prediction on a single slice from X_slices_t\n",
    "    random_slice = X_slices_t[torch.randint(0, len(X_slices_t), (1,)).item()]\n",
    "\n",
    "    # Convert to PyTorch tensor and add batch and channel dimensions\n",
    "    random_slice = torch.tensor(random_slice, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    prediction = model(random_slice).detach()\n",
    "\n",
    "    # Visualize the original and predicted slice\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Original Slice\")\n",
    "    plt.imshow(random_slice.squeeze().cpu().numpy(), cmap='seismic')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Predicted Slice\")\n",
    "    plt.imshow(prediction.squeeze().cpu().numpy(), cmap='seismic')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {running_loss / len(train_loader):.4f}, Train SSIM: {running_ssim / len(train_loader):.4f}, \\\n",
    "          Val Loss: {val_loss / len(val_loader):.4f}, Val SSIM: {val_ssim / len(val_loader):.4f}\")\n",
    "\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
